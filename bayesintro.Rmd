#### Bayesian Methodology and Inferential Applications to Public Policy

#### By Ritika Iyer



### Bayesian Thinking and Methodology:
Before diving into the specifics of the Bayesian regression model, it is important to first understand the theory of conditional probabilities that it relies upon. Let us assume that there are two events, Event A and Event B. The probability that Event A occurs is p(A) and the probability that Event B occurs is p(B). If Event A’s occurrence is unrelated to Event B’s occurrence or non-occurrence, that means the two events are independent. In this case, the joint probability of both events occurring is equal to p(A and B) = p(A)*p(B). 

However, if Event A’s occurrence is conditional upon Event B’s occurrence, then the two events are not independent. A conditional probability is the probability that Event A will happen, given that another event, Event B, already occurred. If we know that Event B happened, then the conditional probability of Event A happening is equal to p(A/B). In this case with conditionality involved, the joint probability of the two events occurring is p(A and B) = p(A)*p(B/A). 

We can use the concepts of joint and conditional probabilities to derive the Bayes theorem, which is p(A/B) = p(A)*p(B/A) / p(B). This equation is interpreted as the probability of Event A occurring given that Event B has occurred is equal to the unconditional probability of Event A occurring times the conditional probability of Event B occurring given that Event A has occurred, divided by the simple unconditional probability of Event B occurring (Inzaugarat, 2018).

#### Bayesian Linear Regression
As discussed above, the main goal of Bayesian Linear Regression is to determine the posterior distribution for the parameters in the model of interest. This is in contrast to frequentist regression models that seek to estimate a single value for each parameter that is considered the best estimate (Koehrsen, 2018). In Bayesian models, the posterior probability of the parameters of interest are conditional upon prior probabilities of their inputs and outputs (Clyde et al., 2020). Equation 2 below shows the fundamental expression of Bayes Theorem, which underlies the Bayesian regression method (Clyde et al., 2020). 

Posterior = (Likelihood * Prior) / Normalization      (2)

In words, we can interpret Equation 2 as the following: the posterior distribution generated from a Bayesian regression model for the parameters is proportional to the likelihood of the parameters data times the prior probability of those parameters (Koehrsen, 2018). Equation 3 below shows how we would start with a Bayesian mode, which assumes that the error term is independent and distributed as normal random variables are (Koehrsen, 2018). 

y = α + βx + ϵ, i = 1,⋯,n     (3)

We can apply the Bayes theorem and probability framework discussed previously to Bayesian regression models, particularly around updating the probability of a hypothesis occurring upon having new data to incorporate in the model (Inzaugarat, 2018). Going back to the Event A and Event B world, we see that the prior, or the probability of the hypothesis before seeing the data, is equal to p(A). The conditional probability of Event A given Event B, or p(A/B), is what we are trying to estimate – the probability of the hypothesis after we see our data, the posterior distribution. The opposite conditional probability of Event B given Event A’s occurrence, equal to p(B/A), is the probability of the data given the hypothesis, or the likelihood estimation. Finally, the simple probability of Event B, p(B), can also be interpreted as the probability of the data under any hypothesis. This probability is also called the normalizing constant under a Bayesian framework (Inzaugarat, 2018).

In a situation where one is comparing two hypotheses or models, the Bayes factor can help quantify the strength and quality of one model over another. The Bayes factor is the ratio of the likelihood probabilities, or the probabilities of the data given the hypothesis, for the two hypotheses of interest (Inzaugarat, 2018). Hypotheses could vary based on the choice of priors, amount of data, and type of priors (informative versus non-informative, for example). 

As compared to frequentist regression, there are a few main benefits of using a Bayesian regression model. Firstly, Bayesian regression leverages priors, which can include educated guesses for model parameters or estimates based on domain knowledge. This is helpful, because compared to frequentist models, we can actually utilize knowledge outside of the data itself to inform our predictions in a positive way (Koehrsen, 2018). Even without helpful priors for all parameters, Bayesian regression allows us to use non-informative priors to improve our model, like assuming a normal distribution. The second benefit of Bayesian regression is that it allows us to quantify how uncertain we are about the model estimates, because it produces a distribution of possibilities for each parameter based on the data used and the priors assumed (Clyde et al., 2020). Based on this, models using smaller data (i.e., with fewer data points) will result in a wider distribution. As the number of data points increases, the parameters converge closer to those estimated from OLS because the likelihood takes over the impact of the priors for each parameter (Clyde et al., 2020). These concepts will be discussed further in the following section.

#### Frequentist versus Bayesian Models
The use of a normative, consistent rule does not apply to frequentist models (Freedman, 1995). However, we can calculate posterior probabilities under the Bayesian framework through the use of Bayes’s Theorem, which allows for consistent methodology across applications. Using density functions, Bayes’s Theorem allows us to draw conclusions about the parameters included in a model by looking at their posterior distribution that is drawn from the data based on their prior distribution. In a Bayesian framework, all parameters are considered to be random variables (in contrast to frequentist frameworks that differentiate between random and fixed parameters) and therefore from a common distribution (Fienberg, 2011). This point raises a common critique of Bayesian techniques, namely questioning the idea that all parameters included in a model are from a single prior distribution. Feinberg points out that most Bayesian analyses report results from the use of various prior distributions to illustrate the role of determining priors in the way the posterior distribution turns out. While some have suggested using a uniform prior across all parameters in a model to avoid skepticism around posterior results, others have claimed that uniform priors actually lack useful information and therefore are often not relevant (Fienberg, 2006). It is also important to note, as discussed earlier, that the more data points there are in an analysis, the less weight is given to the priors and the more weight is given to the data itself in determining the posterior distribution. 

Testing for significance also varies across frequentist and Bayesian methodologies. Using a frequentist inference approach, significance tests are based on the null hypothesis – specifically by calculating a p-value that serves as the computed probability of observing a value at least as extreme as the actual value, conditional on the controls included in the model (Fienberg, 2011). This approach uses the model hypothesis to calculate the probability of the data through repeated sampling. On the other hand, the Bayesian approach uses prior inferences about the covariates based on the data. Here, we estimate the probability of a model hypothesis given the data used (Fienberg, 2011). 

P-values, derived from the frequentist approach, are commonly used in the social science and policy evaluation space. They are often used to inform policy decision-making as well. However, many researchers and officials from the American Statistical Association (ASA) caution that use of p-values can often lead to misinterpretation and misinformation (ASA, 2016; Holzwart et al., 2018). Rather, social science research is increasingly moving toward Bayesian techniques (as an alternative to relying on the p-value approach) to quantify the importance of findings through the use of inference from probabilities. 